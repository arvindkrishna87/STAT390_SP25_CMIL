{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Excel summary written to: patch_summary.xlsx\n"
     ]
    }
   ],
   "source": [
    "### CODE GENERATED USING CHATGPT\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# --- Config ---\n",
    "metrics_dir = 'onedrive_csvs' #change to path to directory containing CSV files\n",
    "patches_csv = 'patches.csv' #CSV containing names of files in Google Drive patch directory\n",
    "annotations_csv = 'annotations.csv' #CSV containing names of files in OneDrive annotation directory\n",
    "patched_csv = 'patched.csv' #CSV containing names of files in OneDrive patched image directory\n",
    "output_excel = 'patch_summary.xlsx' #change to name you want for output file\n",
    "\n",
    "# --- Helper Functions ---\n",
    "#normalizes slice names to account for leading 0s\n",
    "def normalize_slice_id(raw_id): \n",
    "    match = re.match(r\"case_(\\d+)_(un)?match_(\\d+)_(.+)\", raw_id)\n",
    "    if match:\n",
    "        case_num = int(match.group(1))              # Removes leading zero\n",
    "        match_type = \"unmatched\" if match.group(2) == \"un\" else \"match\"\n",
    "        match_num = int(match.group(3))             # Removes leading zero\n",
    "        stain = match.group(4)\n",
    "        return f\"case_{case_num}_{match_type}_{match_num}_{stain}\"\n",
    "    return raw_id\n",
    "\n",
    "#splits filenames into slice, match, stain\n",
    "def parse_slice_components(normalized_id):\n",
    "    match = re.match(r\"case_(\\d+)_(match|unmatched)_(\\d+)_(.+)\", normalized_id)\n",
    "    if match:\n",
    "        case_number = int(match.group(1))\n",
    "        match_type = match.group(2)\n",
    "        match_number = int(match.group(3))\n",
    "        stain = match.group(4)\n",
    "        return case_number, match_type, match_number, stain\n",
    "    return None, None, None, None\n",
    "\n",
    "\n",
    "# --- Step 1: Expected patch counts from metrics_*.csv ---\n",
    "\n",
    "expected_patch_counts = {}\n",
    "\n",
    "for filename in os.listdir(metrics_dir):\n",
    "    if filename.startswith(\"metrics_\") and filename.endswith(\".csv\"):\n",
    "        filepath = os.path.join(metrics_dir, filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "        slice_id = filename.replace(\"metrics_\", \"\").replace(\".csv\", \"\")\n",
    "        num_patches = df.set_index(\"Metric\").loc[\"num_patches\", \"Value\"]\n",
    "        expected_patch_counts[slice_id] = num_patches\n",
    "\n",
    "# --- Step 2: Actual patch counts from patches.csv ---\n",
    "\n",
    "patch_df = pd.read_csv(patches_csv, header=None, names=[\"filename\"])\n",
    "patch_df[\"slice_id\"] = patch_df[\"filename\"].str.extract(r\"^(.*)_patch\\d+\\.png$\")\n",
    "patch_df = patch_df.dropna(subset=[\"slice_id\"])\n",
    "patch_counts = patch_df[\"slice_id\"].value_counts().to_dict()\n",
    "\n",
    "# --- Step 3: Slice-level summary ---\n",
    "\n",
    "all_slices = set(expected_patch_counts) | set(patch_counts)\n",
    "summary_data = []\n",
    "\n",
    "for slice_id in sorted(all_slices):\n",
    "    expected = expected_patch_counts.get(slice_id, 0)\n",
    "    actual = patch_counts.get(slice_id, 0)\n",
    "    normalized_id = normalize_slice_id(slice_id)\n",
    "    summary_data.append({\n",
    "        \"slice_id\": slice_id,\n",
    "        \"normalized_slice_id\": normalized_id,\n",
    "        \"expected_num_patches\": expected,\n",
    "        \"actual_num_patches\": actual,\n",
    "        \"difference\": actual - expected\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# --- Step 4: Load annotation and patched image filenames ---\n",
    "\n",
    "annotations_df = pd.read_csv(annotations_csv, header=None, names=[\"filename\"])\n",
    "annotations_df[\"slice_id\"] = annotations_df[\"filename\"].str.replace(\".png\", \"\", regex=False)\n",
    "annotations_df[\"normalized_slice_id\"] = annotations_df[\"slice_id\"].apply(normalize_slice_id)\n",
    "\n",
    "patched_df = pd.read_csv(patched_csv, header=None, names=[\"filename\"])\n",
    "patched_clean = patched_df[\n",
    "    patched_df[\"filename\"].str.startswith(\"patched_\") &\n",
    "    ~patched_df[\"filename\"].str.contains(\"_patch\")\n",
    "].copy()\n",
    "patched_clean[\"slice_id\"] = (\n",
    "    patched_clean[\"filename\"]\n",
    "    .str.replace(\"patched_\", \"\", regex=False)\n",
    "    .str.replace(\".png\", \"\", regex=False)\n",
    ")\n",
    "patched_clean[\"normalized_slice_id\"] = patched_clean[\"slice_id\"].apply(normalize_slice_id)\n",
    "\n",
    "# --- Step 5: Add annotation and patched flags ---\n",
    "\n",
    "summary_df[\"has_annotation\"] = summary_df[\"normalized_slice_id\"].isin(annotations_df[\"normalized_slice_id\"])\n",
    "summary_df[\"has_patched_image\"] = summary_df[\"normalized_slice_id\"].isin(patched_clean[\"normalized_slice_id\"])\n",
    "\n",
    "# --- Step 6: Parse identifiers into components ---\n",
    "\n",
    "parsed = summary_df[\"normalized_slice_id\"].apply(parse_slice_components)\n",
    "summary_df[[\"case_number\", \"match_type\", \"match_number\", \"stain\"]] = pd.DataFrame(parsed.tolist(), index=summary_df.index)\n",
    "\n",
    "# --- Step 7: Count slices per case_number (only actual > 0) ---\n",
    "\n",
    "slices_per_case = (\n",
    "    summary_df[summary_df[\"actual_num_patches\"] > 0]\n",
    "    .groupby(\"case_number\")\n",
    "    .size()\n",
    "    .reset_index(name=\"num_slices_patched\")\n",
    ")\n",
    "\n",
    "# --- Step 8: Export to Excel ---\n",
    "\n",
    "with pd.ExcelWriter(output_excel, engine=\"openpyxl\") as writer:\n",
    "    summary_df.to_excel(writer, sheet_name=\"summary\", index=False)\n",
    "    slices_per_case.to_excel(writer, sheet_name=\"slices_per_case\", index=False)\n",
    "\n",
    "print(f\"✅ Excel summary written to: {output_excel}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ project_overview.xlsx generated successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load files\n",
    "summary_df = pd.read_excel(\"patch_summary.xlsx\", sheet_name=\"summary\") # the output from cell above\n",
    "slice_patches_df = pd.read_csv(\"case_slices.csv\") # CSV of \"Slices per Case (B/H)\" from Krish's Patching slices spreadsheet\n",
    "\n",
    "# ---- Normalize case identifiers ----\n",
    "def normalize_case_id(val):\n",
    "    match = re.match(r\"^0*(\\d+)$\", str(val))\n",
    "    return int(match.group(1)) if match else val\n",
    "\n",
    "summary_df[\"normalized_case_number\"] = summary_df[\"case_number\"].apply(normalize_case_id)\n",
    "slice_patches_df[\"Case\"] = slice_patches_df[\"Case\"].apply(normalize_case_id)\n",
    "\n",
    "# ---- Determine completion status ----\n",
    "def determine_status(row):\n",
    "    has_annotation = row[\"has_annotation\"]\n",
    "    has_patched_image = row[\"has_patched_image\"]\n",
    "    expected = row[\"expected_num_patches\"]\n",
    "    actual = row[\"actual_num_patches\"]\n",
    "    \n",
    "    if actual == 0 and not has_annotation:\n",
    "        return \"incomplete\"\n",
    "    elif expected == actual and has_annotation and has_patched_image:\n",
    "        return \"complete\"\n",
    "    else:\n",
    "        return \"partial\"\n",
    "\n",
    "\n",
    "summary_df[\"completion_status\"] = summary_df.apply(determine_status, axis=1)\n",
    "\n",
    "# ---- Merge slice data with case data ----\n",
    "case_summary = pd.merge(\n",
    "    slice_patches_df,\n",
    "    summary_df.groupby(\"normalized_case_number\")[\"completion_status\"]\n",
    "        .value_counts()\n",
    "        .unstack(fill_value=0)\n",
    "        .reset_index(),\n",
    "    left_on=\"Case\",\n",
    "    right_on=\"normalized_case_number\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Optional: compute total processed slices\n",
    "case_summary[\"slices_found_in_outputs\"] = case_summary[[\"complete\", \"partial\", \"incomplete\"]].sum(axis=1)\n",
    "\n",
    "# ---- Save results ----\n",
    "with pd.ExcelWriter(\"project_overview.xlsx\") as writer:\n",
    "    summary_df.to_excel(writer, sheet_name=\"Per Slice Summary\", index=False)\n",
    "    case_summary.to_excel(writer, sheet_name=\"Case Summary\", index=False)\n",
    "\n",
    "print(\"✅ project_overview.xlsx generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ project_overview.xlsx updated with expanded summary.\n"
     ]
    }
   ],
   "source": [
    "#updates spreadsheet from cell above to check against expected number of slices per case\n",
    "\n",
    "# ---- Step 1: Generate slice counts per case from summary ----\n",
    "slice_counts_by_case = (\n",
    "    summary_df.groupby(\"normalized_case_number\")[\"completion_status\"]\n",
    "    .value_counts()\n",
    "    .unstack(fill_value=0)\n",
    "    .rename_axis(index=\"Case\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# ---- Step 2: Merge into slice_patches_df ----\n",
    "slice_patches_merged = pd.merge(\n",
    "    slice_patches_df,\n",
    "    slice_counts_by_case,\n",
    "    on=\"Case\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Fill in 0s for any missing complete/partial/incomplete columns\n",
    "for col in [\"complete\", \"partial\", \"incomplete\"]:\n",
    "    if col not in slice_patches_merged:\n",
    "        slice_patches_merged[col] = 0\n",
    "\n",
    "# ---- Step 3: Calculate summary columns ----\n",
    "slice_patches_merged[\"actual_total_slices_found\"] = (\n",
    "    slice_patches_merged[\"complete\"] +\n",
    "    slice_patches_merged[\"partial\"] +\n",
    "    slice_patches_merged[\"incomplete\"]\n",
    ")\n",
    "\n",
    "slice_patches_merged[\"slices_missing_from_output\"] = (\n",
    "    slice_patches_merged[\"Total slices\"] - slice_patches_merged[\"actual_total_slices_found\"]\n",
    ")\n",
    "\n",
    "# ---- Step 4: Save updated Excel ----\n",
    "with pd.ExcelWriter(\"project_overview.xlsx\") as writer:\n",
    "    summary_df.to_excel(writer, sheet_name=\"Per Slice Summary\", index=False)\n",
    "    slice_patches_merged.to_excel(writer, sheet_name=\"Expanded Case Summary\", index=False)\n",
    "    print(\"✅ project_overview.xlsx updated with expanded summary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
